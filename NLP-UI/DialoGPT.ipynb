{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total    : 2.101870592\n",
      "free     : 1.72097536\n",
      "used     : 0.380895232\n"
     ]
    }
   ],
   "source": [
    "from pynvml import *\n",
    "nvmlInit()\n",
    "h = nvmlDeviceGetHandleByIndex(0)\n",
    "info = nvmlDeviceGetMemoryInfo(h)\n",
    "print(f'total    : {info.total/1000000000}')\n",
    "print(f'free     : {info.free/1000000000}')\n",
    "print(f'used     : {info.used/1000000000}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# import huggingface transformers\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, GPT2Config, AdamW\n",
    "\n",
    "def top_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k, top-p (nucleus) and/or threshold filtering\n",
    "        Args:\n",
    "            logits: logits distribution shape (vocabulary size)\n",
    "            top_k: <=0: no filtering, >0: keep only top k tokens with highest probability.\n",
    "            top_p: <=0.0: no filtering, >0.0: keep only a subset S of candidates, where S is the smallest subset\n",
    "                whose total probability mass is greater than or equal to the threshold top_p.\n",
    "                In practice, we select the highest probability tokens whose cumulative probability mass exceeds\n",
    "                the threshold top_p.\n",
    "    \"\"\"\n",
    "    # batch support!\n",
    "    if top_k > 0:\n",
    "        values, _ = torch.topk(logits, top_k)\n",
    "        min_values = values[:, -1].unsqueeze(1).repeat(1, logits.shape[-1])\n",
    "        logits = torch.where(logits < min_values, \n",
    "                             torch.ones_like(logits, dtype=logits.dtype) * -float('Inf'), \n",
    "                             logits)\n",
    "    if top_p > 0.0:\n",
    "        # Compute cumulative probabilities of sorted tokens\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probabilities = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probabilities > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "        \n",
    "        sorted_logits = sorted_logits.masked_fill_(sorted_indices_to_remove, filter_value)\n",
    "        logits = torch.zeros_like(logits).scatter(1, sorted_indices, sorted_logits)\n",
    "    \n",
    "    return logits\n",
    "\n",
    "gpt2_large_config = GPT2Config(n_ctx=1024, n_embd=1280, n_layer=36, n_head=20)   \n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "model = GPT2LMHeadModel(gpt2_large_config)\n",
    "model.load_state_dict(torch.load(\"dialoGPT_large_ft.pkl\"), strict=False)\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "model.lm_head.weight.data = model.transformer.wte.weight.data\n",
    "\n",
    "eos = [tokenizer.encoder[\"<|endoftext|>\"]]\n",
    "\n",
    "past = None\n",
    "temperature = 0.9\n",
    "top_k = -1\n",
    "top_p = 0.9\n",
    "\n",
    "model.eval()\n",
    "prev_input = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User: hello ENJOY!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: Hey. lol\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User: get me details on Rebecca!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: I don't know if I have the answers for that, I just want to know you.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User: quit\n"
     ]
    }
   ],
   "source": [
    "## Check input for chatbot\n",
    "while True:\n",
    "    with torch.no_grad():\n",
    "        # input and update B's utterance\n",
    "        user = input(\"User:\")\n",
    "        \n",
    "        if user == \"quit\":\n",
    "            \"stop talking!\"\n",
    "            break\n",
    "        \n",
    "        user = tokenizer.encode(user)\n",
    "        prev_input = user\n",
    "        prev_input = torch.LongTensor(prev_input).unsqueeze(0).to(device)\n",
    "        _, past = model(prev_input, past=past)\n",
    "\n",
    "        prev_input = torch.LongTensor([eos]).to(device)\n",
    "    \n",
    "\n",
    "        sent = []\n",
    "        for i in range(500):\n",
    "            logits, past = model(prev_input, past=past)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            logits = top_filtering(logits, top_k=top_k, top_p=top_p)\n",
    "\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "            prev_input = torch.multinomial(probs, num_samples=1)\n",
    "            prev_word = prev_input.item()\n",
    "\n",
    "            if prev_word == eos[0]:\n",
    "                break\n",
    "            sent.append(prev_word)\n",
    "        \n",
    "        print(\"Bot:\", tokenizer.decode(sent))\n",
    "        prev_input = torch.LongTensor([eos]).to(device)\n",
    "        _, past = model(prev_input, past=past)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

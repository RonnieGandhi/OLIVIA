{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QA:\n",
    "    def __init__(self,model_path: str):\n",
    "        self.max_seq_length = 384\n",
    "        self.doc_stride = 128\n",
    "        self.do_lower_case = True\n",
    "        self.max_query_length = 64\n",
    "        self.n_best_size = 20\n",
    "        self.max_answer_length = 30\n",
    "        self.model, self.tokenizer = self.load_model(model_path)\n",
    "#         if torch.cuda.is_available():\n",
    "#             self.device = 'cuda'\n",
    "#         else:\n",
    "#             self.device = 'cpu'\n",
    "        self.device = 'cpu'\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "\n",
    "\n",
    "    def load_model(self,model_path: str,do_lower_case=False):\n",
    "#        config = BertConfig.from_pretrained(model_path + \"/bert_config.json\")\n",
    "#         tokenizer = BertTokenizer.from_pretrained(model_path, do_lower_case=do_lower_case)\n",
    "#         model = BertForQuestionAnswering.from_pretrained(model_path, from_tf=False, config=config)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"deepset/bert-large-uncased-whole-word-masking-squad2\")\n",
    "        model = AutoModelForQuestionAnswering.from_pretrained(\"deepset/bert-large-uncased-whole-word-masking-squad2\")\n",
    "        return model, tokenizer\n",
    "    \n",
    "    def predict(self,passage :str,question :str):\n",
    "        example = input_to_squad_example(passage,question)\n",
    "        features = squad_examples_to_features(example,self.tokenizer,self.max_seq_length,self.doc_stride,self.max_query_length)\n",
    "        all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "        all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "        all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "        all_example_index = torch.arange(all_input_ids.size(0), dtype=torch.long)\n",
    "        dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids,\n",
    "                                all_example_index)\n",
    "        eval_sampler = SequentialSampler(dataset)\n",
    "        eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=1)\n",
    "        all_results = []\n",
    "        for batch in eval_dataloader:\n",
    "            batch = tuple(t.to(self.device) for t in batch)\n",
    "            with torch.no_grad():\n",
    "                inputs = {'input_ids':      batch[0],\n",
    "                        'attention_mask': batch[1],\n",
    "                        'token_type_ids': batch[2]  \n",
    "                        }\n",
    "                example_indices = batch[3]\n",
    "                outputs = self.model(**inputs)\n",
    "\n",
    "            for i, example_index in enumerate(example_indices):\n",
    "                eval_feature = features[example_index.item()]\n",
    "                unique_id = int(eval_feature.unique_id)\n",
    "                result = RawResult(unique_id    = unique_id,\n",
    "                                    start_logits = to_list(outputs[0][i]),\n",
    "                                    end_logits   = to_list(outputs[1][i]))\n",
    "                all_results.append(result)\n",
    "        answer = get_answer(example,features,all_results,self.n_best_size,self.max_answer_length,self.do_lower_case)\n",
    "        return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import collections\n",
    "import logging\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from pytorch_transformers import (WEIGHTS_NAME, BertConfig,\n",
    "                                  BertForQuestionAnswering, BertTokenizer)\n",
    "from torch.utils.data import DataLoader, SequentialSampler, TensorDataset\n",
    "\n",
    "from utils import (get_answer, input_to_squad_example,\n",
    "                   squad_examples_to_features, to_list)\n",
    "\n",
    "RawResult = collections.namedtuple(\"RawResult\",\n",
    "                                   [\"unique_id\", \"start_logits\", \"end_logits\"])\n",
    "\n",
    "modelQA = QA('model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new state-of-the-art results on eleven natural language processing tasks\n",
      "86.7%\n",
      "93.2\n"
     ]
    }
   ],
   "source": [
    "doc = 'BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE benchmark to 80.4% (7.6% absolute improvement), MultiNLI accuracy to 86.7% (5.6% absolute improvement) and the SQuAD v1.1 question answering Test F1 to 93.2 (1.5 absolute improvement), outperforming human performance by 2.0.'\n",
    "\n",
    "q1 =  'What does BERT achieves ?'\n",
    "q2 = \"What is the BERT's accuracy on MultiNLI ?\"\n",
    "q3 = \"What is Bert's F1 score on Squad v1.1 ?\"\n",
    "\n",
    "a1 = modelQA.predict(doc,q1)\n",
    "a2 = modelQA.predict(doc,q2)\n",
    "a3 = modelQA.predict(doc,q3)\n",
    "\n",
    "print(a1['answer'])\n",
    "print(a2['answer'])\n",
    "print(a3['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4261209550639394\n",
      "0.9372814687877098\n",
      "0.943220679668327\n"
     ]
    }
   ],
   "source": [
    "print(a1['confidence'])\n",
    "print(a2['confidence'])\n",
    "print(a3['confidence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
